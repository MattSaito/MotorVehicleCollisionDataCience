{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0be552d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import os\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af6cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Motor_Vehicle_Collisions_-_Crashes_20251015.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd0e613",
   "metadata": {},
   "source": [
    "# Lidando com Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b40bf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum() # Muitos valores nulos impossivel exclui-los"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730d8de",
   "metadata": {},
   "source": [
    "## ~~Removendo Colunas com dados nulos acima de 82%~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a6d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum() # colunas off street name, contributing factor 3 veicle ate o 5 , vehicle type code 3 ate o 5 sao colunas com muitos valores nulos e podem ser removidas\n",
    "missing_percent = df.isna().mean() * 100\n",
    "print(missing_percent.sort_values(ascending=False))\n",
    "# df_removed = df.drop(columns=['OFF STREET NAME', 'CONTRIBUTING FACTOR VEHICLE 3', 'CONTRIBUTING FACTOR VEHICLE 4', 'CONTRIBUTING FACTOR VEHICLE 5', 'VEHICLE TYPE CODE 3', 'VEHICLE TYPE CODE 4', 'VEHICLE TYPE CODE 5'])\n",
    "# df_removed.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bfbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparando antes e depois de remover os nulos\n",
    "# print(df.shape)\n",
    "# print(df_removed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a9cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # demonstração de valores faltantes em ordem descendente\n",
    "# df_removed.isna().sum() \n",
    "# missing_percent = df_removed.isna().mean() * 100\n",
    "# print(missing_percent.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4b9fa",
   "metadata": {},
   "source": [
    "# Lidando Com Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df779ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte os outliars de longitude e latitude em Nan\n",
    "lat_min, lat_max = 40.49, 40.92\n",
    "lon_min, lon_max = -74.26, -73.69\n",
    "filtro_invalidas = (\n",
    "    (df['LATITUDE'] < lat_min) | (df['LATITUDE'] > lat_max) |\n",
    "    (df['LONGITUDE'] < lon_min) | (df['LONGITUDE'] > lon_max)\n",
    ")\n",
    "df.loc[filtro_invalidas, ['LATITUDE', 'LONGITUDE']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9557262f",
   "metadata": {},
   "source": [
    "## Usando novo dataset sem colunas com muitos nulos para imputar restante dos resultados com simpleImputer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_auto = df.select_dtypes(include=['number']).columns.to_list()\n",
    "cat_cols_auto = df.select_dtypes(include=['object']).columns.to_list()\n",
    "num_cols_imputar = [\n",
    "    col for col in num_cols_auto \n",
    "    if col not in ['LATITUDE', 'LONGITUDE', 'COLLISION_ID'] \n",
    "]\n",
    "cat_cols_imputar = cat_cols_auto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb30e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers_imput = [\n",
    "    ('num_imputer', SimpleImputer(strategy='median'), num_cols_imputar),\n",
    "    ('freq_imputer', SimpleImputer(strategy='most_frequent'), cat_cols_imputar)\n",
    "]\n",
    "ct_imput = ColumnTransformer(transformers_imput, remainder='passthrough')\n",
    "df_imputed_array = ct_imput.fit_transform(df)\n",
    "\n",
    "# Obter nomes das colunas 'remainder'\n",
    "imputed_cols = num_cols_imputar + cat_cols_imputar\n",
    "all_cols_original = df.columns.tolist()\n",
    "remainder_cols = [col for col in all_cols_original if col not in imputed_cols]\n",
    "\n",
    "# Ordem correta das colunas no array\n",
    "nova_ordem_colunas = num_cols_imputar + cat_cols_imputar + remainder_cols\n",
    "\n",
    "# Reconstruir\n",
    "df_imputed = pd.DataFrame(df_imputed_array, columns=nova_ordem_colunas)\n",
    "\n",
    "colunas_numericas_finais = num_cols_imputar + remainder_cols\n",
    "\n",
    "for col in colunas_numericas_finais:\n",
    "    # pd.to_numeric é a melhor forma de fazer isso\n",
    "    df_imputed[col] = pd.to_numeric(df_imputed[col], errors='coerce')\n",
    "\n",
    "# 2. (Opcional) Garante que as colunas categóricas sejam 'string'\n",
    "for col in cat_cols_imputar:\n",
    "    df_imputed[col] = df_imputed[col].astype(str)\n",
    "\n",
    "print(\"\\n--- DEPOIS da Conversão de Tipo (Tipos Corrigidos) ---\")\n",
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da081b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.isna().sum() \n",
    "missing_percent = df_imputed.isna().mean() * 100\n",
    "print(missing_percent.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1946165",
   "metadata": {},
   "source": [
    "# Resolvendo inconsistências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ecc150",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # df = pd.read_csv('df_8_14_colunas.csv')\n",
    "    # print(f\"--- Carregado 'df_8_14_colunas.csv' para Identificação ---\")\n",
    "    \n",
    "    # 1. Identificar todas as colunas que são do tipo 'object' (texto)\n",
    "    object_cols = df_imputed.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    if len(object_cols) == 0:\n",
    "        print(\"Nenhuma coluna de texto (object) encontrada neste arquivo.\")\n",
    "    else:\n",
    "        print(f\"Colunas de texto (object) encontradas: {list(object_cols)}\")\n",
    "        \n",
    "        # Limite para decidir se imprimimos todos os valores únicos\n",
    "        low_cardinality_threshold = 50 \n",
    "        \n",
    "        print(\"\\n--- 2. Análise de Valores Únicos (Identificação) ---\")\n",
    "        \n",
    "        for col in object_cols:\n",
    "            num_unique = df[col].nunique()\n",
    "            \n",
    "            print(f\"\\nColuna: '{col}' | Total de Valores Únicos: {num_unique}\")\n",
    "            \n",
    "            # Se a coluna tiver poucos valores únicos, é categórica.\n",
    "            # Vamos imprimir as contagens para o usuário ver.\n",
    "            if num_unique <= low_cardinality_threshold:\n",
    "                print(\"Contagem de valores (value_counts):\")\n",
    "                # Imprimir .value_counts() que já vem ordenado\n",
    "                print(df[col].value_counts(dropna=False)) # dropna=False para vermos os NaNs\n",
    "            else:\n",
    "                print(f\"Cardinalidade muito alta. Exibindo 5 valores de exemplo:\")\n",
    "                print(df[col].sample(5, random_state=1).to_list())\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo 'df_8_14_colunas.csv' não foi encontrado.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro inesperado: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafe268",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduzido = df_imputed.iloc[:, 14:16]\n",
    "df_reduzido.to_csv(\"df_14_16_colunas.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0441c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4256bb",
   "metadata": {},
   "source": [
    "# Resolvendo Tipos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b465ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a23b999",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_typed = df_imputed.copy()\n",
    "\n",
    "# Convert all columns that start with \"NUMBER\" to int\n",
    "number_cols = [col for col in df_typed.columns if col.startswith(\"NUMBER\")]\n",
    "for col in number_cols:\n",
    "    df_typed[col] = pd.to_numeric(df_typed[col], errors='coerce').astype('Int64')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_typed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7565da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check date-related columns\n",
    "date_cols = [col for col in df_typed.columns if 'DATE' in col.upper() or 'TIME' in col.upper()]\n",
    "print(\"Date/Time columns found:\")\n",
    "print(date_cols)\n",
    "print(\"\\nSample values:\")\n",
    "for col in date_cols:\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(df_typed[col].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3222bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine CRASH DATE and CRASH TIME into a single datetime column\n",
    "df_typed['CRASH_DATETIME'] = pd.to_datetime(\n",
    "    df_typed['CRASH DATE'] + ' ' + df_typed['CRASH TIME'], \n",
    "    format='%m/%d/%Y %H:%M',\n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "# Display the result\n",
    "print(\"New CRASH_DATETIME column created:\")\n",
    "print(df_typed[['CRASH DATE', 'CRASH TIME', 'CRASH_DATETIME']].head(10))\n",
    "print(f\"\\nData type: {df_typed['CRASH_DATETIME'].dtype}\")\n",
    "print(f\"Null values: {df_typed['CRASH_DATETIME'].isna().sum()}\")\n",
    "\n",
    "# Drop the original date and time columns\n",
    "df_typed = df_typed.drop(columns=['CRASH DATE', 'CRASH TIME'])\n",
    "print(\"\\n--- After dropping CRASH DATE and CRASH TIME ---\")\n",
    "print(df_typed.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c583966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all object columns to explicit string type\n",
    "object_cols = df_typed.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"Converting {len(object_cols)} object columns to string type:\")\n",
    "print(object_cols)\n",
    "\n",
    "for col in object_cols:\n",
    "    df_typed[col] = df_typed[col].astype('string')\n",
    "\n",
    "print(\"\\n--- After converting objects to strings ---\")\n",
    "print(df_typed.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48ce9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#versao sem ID\n",
    "df_typed = df_typed.drop(columns = ['LOCATION'])\n",
    "df_noid = df_typed.drop(columns=['COLLISION_ID'])\n",
    "print(df_noid)\n",
    "df_noid.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65af8d3e",
   "metadata": {},
   "source": [
    "# SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e157d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recarregar dados brutos para comparação\n",
    "# df_raw: dados originais para comparar (sem imputação/outlier fix)\n",
    "df_raw = pd.read_csv(\"../Motor_Vehicle_Collisions_-_Crashes_20251015.csv\")\n",
    "print(\"Linhas raw:\", len(df_raw))\n",
    "print(\"Linhas clean (df_typed):\", len(df_typed))\n",
    "\n",
    "# Bounding box NYC utilizado na limpeza\n",
    "lat_min, lat_max = 40.49, 40.92\n",
    "lon_min, lon_max = -74.26, -73.69\n",
    "\n",
    "# Marcar outliers nas coordenadas do raw (antes de limpeza)\n",
    "raw_outlier_mask = (\n",
    "    (df_raw['LATITUDE'] < lat_min) | (df_raw['LATITUDE'] > lat_max) |\n",
    "    (df_raw['LONGITUDE'] < lon_min) | (df_raw['LONGITUDE'] > lon_max)\n",
    ")\n",
    "print(\"Outliers LAT/LON no raw:\", raw_outlier_mask.sum())\n",
    "print(\"NAN LAT/LON já no clean:\", df_typed['LATITUDE'].isna().sum(), df_typed['LONGITUDE'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672114c5",
   "metadata": {},
   "source": [
    "## Resumo da Validação da Limpeza\n",
    "Principais achados dos dados brutos vs dados limpos:\n",
    "\n",
    "1. Valores Nulos:\n",
    "   - Conjunto bruto apresenta ~10–20% de nulos em colunas secundárias (VEHICLE TYPE CODE 2, CONTRIBUTING FACTOR VEHICLE 2) e ~11% em LATITUDE/LONGITUDE (após marcação de inválidos).\n",
    "   - Conjunto limpo mantém proporções controladas nas colunas chave e converte coordenadas inválidas explicitamente em NaN para tratamento downstream.\n",
    "2. Outliers Geográficos:\n",
    "   - Tínhamos 6.485 registros fora do bounding box (NYC). Visualização mostra pontos dispersos irreais (ex.: longitudes extremas). Após limpeza, distribuição concentra-se corretamente sobre NYC.\n",
    "3. Tipagem e Consistência:\n",
    "   - Colunas numéricas convertidas para tipos inteiros (nullable Int64) garantindo coerência para agregações.\n",
    "   - Datas unificadas em `CRASH_DATETIME` permitindo análises temporais diretas (evolução mensal preservada).\n",
    "4. Fatores Contributivos & Tipos de Veículo:\n",
    "   - Ranking top de fatores foi preservado (ex.: Driver Inattention/Distraction, Failure to Yield). Heatmap evidencia relação fator x tipo: sedans e passenger vehicles predominam nos fatores inespecíficos e de distração.\n",
    "5. Evolução Temporal:\n",
    "   - Série mensal consistente entre cru e limpo, indicando que transformação não distorceu volume global por período; quedas visíveis relativas à pandemia permanecem.\n",
    "6. Redução de Risco Analítico:\n",
    "   - Remoção/isolamento de outliers e padronização de tipos reduz vieses em modelos futuros (ex.: clustering por área ou estimativa de severidade).\n",
    "\n",
    "Conclusão: A limpeza aplicada melhora a confiabilidade espacial e de consistência de tipos/date-time sem alterar tendências globais; próximos passos podem incluir normalização semântica adicional (ex.: agrupar fatores raros em 'Other') e enriquecimento geográfico (reverse geocoding de LAT/LON válidos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar conexão SQLite em memória e registrar tabelas reduzidas para eficiência\n",
    "\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "cols_interest = [\n",
    "    'CRASH_DATETIME', 'LATITUDE', 'LONGITUDE',\n",
    "    'NUMBER OF PERSONS INJURED','NUMBER OF PERSONS KILLED',\n",
    "    'NUMBER OF PEDESTRIANS INJURED','NUMBER OF PEDESTRIANS KILLED',\n",
    "    'NUMBER OF CYCLIST INJURED','NUMBER OF CYCLIST KILLED',\n",
    "    'NUMBER OF MOTORIST INJURED','NUMBER OF MOTORIST KILLED',\n",
    "    'CONTRIBUTING FACTOR VEHICLE 1','CONTRIBUTING FACTOR VEHICLE 2',\n",
    "    'VEHICLE TYPE CODE 1','VEHICLE TYPE CODE 2'\n",
    "]\n",
    "\n",
    "# Preparar raw com datetime reconstruído\n",
    "raw_dt = pd.to_datetime(df_raw['CRASH DATE'] + ' ' + df_raw['CRASH TIME'], errors='coerce')\n",
    "df_raw_sql = df_raw.assign(CRASH_DATETIME=raw_dt)[cols_interest]\n",
    "\n",
    "df_clean_sql = df_typed[cols_interest].copy()\n",
    "\n",
    "# Registrar\n",
    "%time df_raw_sql.to_sql('crashes_raw', conn, index=False)\n",
    "%time df_clean_sql.to_sql('crashes_clean', conn, index=False)\n",
    "\n",
    "print('Tabelas criadas: crashes_raw, crashes_clean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8b60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consultas SQL diagnósticas\n",
    "queries = {\n",
    "    'count_rows_raw': \"SELECT COUNT(*) AS total_raw FROM crashes_raw;\",\n",
    "    'count_rows_clean': \"SELECT COUNT(*) AS total_clean FROM crashes_clean;\",\n",
    "    'nulls_raw': \"\"\"\n",
    "        SELECT column_name,\n",
    "               SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) AS nulls,\n",
    "               COUNT(*) AS total,\n",
    "               ROUND(100.0*SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END)/COUNT(*),2) AS pct_null\n",
    "        FROM (\n",
    "          SELECT CRASH_DATETIME AS value, 'CRASH_DATETIME' AS column_name FROM crashes_raw UNION ALL\n",
    "          SELECT LATITUDE,'LATITUDE' FROM crashes_raw UNION ALL\n",
    "          SELECT LONGITUDE,'LONGITUDE' FROM crashes_raw UNION ALL\n",
    "          SELECT `NUMBER OF PERSONS INJURED`,'NUMBER OF PERSONS INJURED' FROM crashes_raw UNION ALL\n",
    "          SELECT `NUMBER OF PERSONS KILLED`,'NUMBER OF PERSONS KILLED' FROM crashes_raw UNION ALL\n",
    "          SELECT `CONTRIBUTING FACTOR VEHICLE 1`,'CONTRIBUTING FACTOR VEHICLE 1' FROM crashes_raw UNION ALL\n",
    "          SELECT `CONTRIBUTING FACTOR VEHICLE 2`,'CONTRIBUTING FACTOR VEHICLE 2' FROM crashes_raw UNION ALL\n",
    "          SELECT `VEHICLE TYPE CODE 1`,'VEHICLE TYPE CODE 1' FROM crashes_raw UNION ALL\n",
    "          SELECT `VEHICLE TYPE CODE 2`,'VEHICLE TYPE CODE 2' FROM crashes_raw\n",
    "        ) t\n",
    "        GROUP BY column_name ORDER BY pct_null DESC;\"\"\",\n",
    "    'nulls_clean': \"\"\"\n",
    "        SELECT column_name,\n",
    "               SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END) AS nulls,\n",
    "               COUNT(*) AS total,\n",
    "               ROUND(100.0*SUM(CASE WHEN value IS NULL THEN 1 ELSE 0 END)/COUNT(*),2) AS pct_null\n",
    "        FROM (\n",
    "          SELECT CRASH_DATETIME AS value, 'CRASH_DATETIME' AS column_name FROM crashes_clean UNION ALL\n",
    "          SELECT LATITUDE,'LATITUDE' FROM crashes_clean UNION ALL\n",
    "          SELECT LONGITUDE,'LONGITUDE' FROM crashes_clean UNION ALL\n",
    "          SELECT `NUMBER OF PERSONS INJURED`,'NUMBER OF PERSONS INJURED' FROM crashes_clean UNION ALL\n",
    "          SELECT `NUMBER OF PERSONS KILLED`,'NUMBER OF PERSONS KILLED' FROM crashes_clean UNION ALL\n",
    "          SELECT `CONTRIBUTING FACTOR VEHICLE 1`,'CONTRIBUTING FACTOR VEHICLE 1' FROM crashes_clean UNION ALL\n",
    "          SELECT `CONTRIBUTING FACTOR VEHICLE 2`,'CONTRIBUTING FACTOR VEHICLE 2' FROM crashes_clean UNION ALL\n",
    "          SELECT `VEHICLE TYPE CODE 1`,'VEHICLE TYPE CODE 1' FROM crashes_clean UNION ALL\n",
    "          SELECT `VEHICLE TYPE CODE 2`,'VEHICLE TYPE CODE 2' FROM crashes_clean\n",
    "        ) t\n",
    "        GROUP BY column_name ORDER BY pct_null DESC;\"\"\",\n",
    "    'top_contributing_factors_raw': \"\"\"\n",
    "        SELECT `CONTRIBUTING FACTOR VEHICLE 1` AS factor, COUNT(*) AS cnt\n",
    "        FROM crashes_raw\n",
    "        WHERE `CONTRIBUTING FACTOR VEHICLE 1` IS NOT NULL AND TRIM(`CONTRIBUTING FACTOR VEHICLE 1`) <> ''\n",
    "        GROUP BY factor ORDER BY cnt DESC LIMIT 15;\"\"\",\n",
    "    'top_contributing_factors_clean': \"\"\"\n",
    "        SELECT `CONTRIBUTING FACTOR VEHICLE 1` AS factor, COUNT(*) AS cnt\n",
    "        FROM crashes_clean\n",
    "        WHERE `CONTRIBUTING FACTOR VEHICLE 1` IS NOT NULL AND TRIM(`CONTRIBUTING FACTOR VEHICLE 1`) <> ''\n",
    "        GROUP BY factor ORDER BY cnt DESC LIMIT 15;\"\"\",\n",
    "    'vehicle_types_raw': \"\"\"\n",
    "        SELECT `VEHICLE TYPE CODE 1` AS vtype, COUNT(*) AS cnt\n",
    "        FROM crashes_raw\n",
    "        WHERE `VEHICLE TYPE CODE 1` IS NOT NULL AND TRIM(`VEHICLE TYPE CODE 1`) <> ''\n",
    "        GROUP BY vtype ORDER BY cnt DESC LIMIT 15;\"\"\",\n",
    "    'vehicle_types_clean': \"\"\"\n",
    "        SELECT `VEHICLE TYPE CODE 1` AS vtype, COUNT(*) AS cnt\n",
    "        FROM crashes_clean\n",
    "        WHERE `VEHICLE TYPE CODE 1` IS NOT NULL AND TRIM(`VEHICLE TYPE CODE 1`) <> ''\n",
    "        GROUP BY vtype ORDER BY cnt DESC LIMIT 15;\"\"\",\n",
    "    'monthly_counts_raw': \"\"\"\n",
    "        SELECT STRFTIME('%Y-%m', CRASH_DATETIME) AS ym, COUNT(*) AS cnt\n",
    "        FROM crashes_raw\n",
    "        WHERE CRASH_DATETIME IS NOT NULL\n",
    "        GROUP BY ym ORDER BY ym;\"\"\",\n",
    "    'monthly_counts_clean': \"\"\"\n",
    "        SELECT STRFTIME('%Y-%m', CRASH_DATETIME) AS ym, COUNT(*) AS cnt\n",
    "        FROM crashes_clean\n",
    "        WHERE CRASH_DATETIME IS NOT NULL\n",
    "        GROUP BY ym ORDER BY ym;\"\"\"\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, q in queries.items():\n",
    "    results[name] = pd.read_sql_query(q, conn)\n",
    "    print(f\"--- {name} ---\")\n",
    "    print(results[name].head())\n",
    "    print()\n",
    "\n",
    "# Guardar para uso nas visualizações\n",
    "monthly_raw = results['monthly_counts_raw']\n",
    "monthly_clean = results['monthly_counts_clean']\n",
    "nulls_raw = results['nulls_raw']\n",
    "nulls_clean = results['nulls_clean']\n",
    "veh_raw = results['vehicle_types_raw']\n",
    "veh_clean = results['vehicle_types_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03141e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Missing % comparação\n",
    "miss_compare = nulls_raw[['column_name','pct_null']].merge(\n",
    "    nulls_clean[['column_name','pct_null']], on='column_name', suffixes=('_raw','_clean')\n",
    ")\n",
    "plt.figure(figsize=(10,5))\n",
    "miss_compare_melt = miss_compare.melt(id_vars='column_name', value_vars=['pct_null_raw','pct_null_clean'],\n",
    "                                      var_name='dataset', value_name='pct_null')\n",
    "sns.barplot(data=miss_compare_melt, x='column_name', y='pct_null', hue='dataset')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Percentual de Nulos: Raw vs Clean')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evolução mensal\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(monthly_raw['ym'], monthly_raw['cnt'], label='Raw', alpha=0.6)\n",
    "plt.plot(monthly_clean['ym'], monthly_clean['cnt'], label='Clean', alpha=0.6)\n",
    "plt.xticks(monthly_clean['ym'][::max(len(monthly_clean)//12,1)], rotation=45)\n",
    "plt.title('Contagem Mensal de Colisões')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top tipos de veículo antes/depois\n",
    "veh_merge = veh_raw.merge(veh_clean, on='vtype', suffixes=('_raw','_clean'))\n",
    "veh_merge['diff'] = veh_merge['cnt_clean'] - veh_merge['cnt_raw']\n",
    "plt.figure(figsize=(10,6))\n",
    "veh_merge_sorted = veh_merge.sort_values('cnt_clean', ascending=False).head(15)\n",
    "sns.barplot(data=veh_merge_sorted.melt(id_vars='vtype', value_vars=['cnt_raw','cnt_clean']), x='vtype', y='value', hue='variable')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Top 15 Tipos de Veículo: Raw vs Clean')\n",
    "plt.ylabel('Contagem')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Coordenadas: distribuição (amostra para performance)\n",
    "sample_raw = df_raw_sql[['LATITUDE','LONGITUDE']].dropna().sample(20000, random_state=42) if len(df_raw_sql)>20000 else df_raw_sql[['LATITUDE','LONGITUDE']].dropna()\n",
    "sample_clean = df_clean_sql[['LATITUDE','LONGITUDE']].dropna().sample(20000, random_state=42) if len(df_clean_sql)>20000 else df_clean_sql[['LATITUDE','LONGITUDE']].dropna()\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "axes[0].scatter(sample_raw['LONGITUDE'], sample_raw['LATITUDE'], s=3, alpha=0.3)\n",
    "axes[0].set_title('Raw Coordenadas (amostra)')\n",
    "axes[1].scatter(sample_clean['LONGITUDE'], sample_clean['LATITUDE'], s=3, alpha=0.3, color='green')\n",
    "axes[1].set_title('Clean Coordenadas (amostra)')\n",
    "for ax in axes:\n",
    "    ax.set_xlabel('Longitude'); ax.set_ylabel('Latitude')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "# Heatmap fatores x tipos (clean)\n",
    "factor_type = pd.read_sql_query(\"\"\"\n",
    "    SELECT `CONTRIBUTING FACTOR VEHICLE 1` AS factor, `VEHICLE TYPE CODE 1` AS vtype\n",
    "    FROM crashes_clean\n",
    "    WHERE factor IS NOT NULL AND vtype IS NOT NULL AND TRIM(factor)<>'' AND TRIM(vtype)<>''\n",
    "\"\"\", conn)\n",
    "# limitar top para evitar matriz enorme\n",
    "top_factors = factor_type['factor'].value_counts().head(10).index\n",
    "top_vtypes = factor_type['vtype'].value_counts().head(10).index\n",
    "matrix_df = factor_type[factor_type['factor'].isin(top_factors) & factor_type['vtype'].isin(top_vtypes)]\n",
    "pivot = matrix_df.pivot_table(index='factor', columns='vtype', aggfunc=len, fill_value=0)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.heatmap(pivot, cmap='Blues')\n",
    "plt.title('Heatmap Fatores vs Tipos (Top10) - Clean')\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf395c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
